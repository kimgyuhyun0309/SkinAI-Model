"""
metadata_builder.py
- SkinAI í”„ë¡œì íŠ¸ì˜ ì›ë³¸ ì´ë¯¸ì§€ë¡œë¶€í„°
  1) manifest_raw.tsv : íŒŒì¼ ê²½ë¡œ + í´ë˜ìŠ¤ + SHA-1 í•´ì‹œ
  2) normalized ì´ë¯¸ì§€ ì €ì¥ + manifest_normalized.tsv
  3) blur(íë¦¿í•¨) ì œê±° + manifest_clean.tsv

  ì´ ëª¨ë“  ê³¼ì •ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

import csv
import hashlib
import pandas as pd
import numpy as np
from pathlib import Path
from PIL import Image
import cv2

from config import PROJECT_ROOT, CLASSES


# ============================================================
# 1) ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
# ============================================================
SOURCE_DIR = PROJECT_ROOT / "images"
MANIFEST_RAW = PROJECT_ROOT / "manifest_raw.tsv"
MANIFEST_NORMALIZED = PROJECT_ROOT / "manifest_normalized.tsv"
MANIFEST_CLEAN = PROJECT_ROOT / "manifest_clean.tsv"

NORMALIZED_DIR = PROJECT_ROOT / "normalized"
NORMALIZED_DIR.mkdir(exist_ok=True)


# ============================================================
# 2) SHA-1 í•´ì‹œ í•¨ìˆ˜
# ============================================================
def sha1(path: Path) -> str:
    """íŒŒì¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ SHA-1 í•´ì‹œ ìƒì„±"""
    h = hashlib.sha1()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1 << 20), b""):
            h.update(chunk)
    return h.hexdigest()


# ============================================================
# 3) Step 1 â€” manifest_raw.tsv ìƒì„±
# ============================================================
def build_manifest_raw():

    rows = []
    exts = (".jpg", ".jpeg", ".png", ".bmp", ".webp")

    for cls in CLASSES:
        cls_dir = SOURCE_DIR / cls
        for ext in exts:
            for p in cls_dir.rglob(f"*{ext}"):
                rows.append({
                    "filepath": str(p),
                    "class": cls,
                    "hash": sha1(p)
                })

    with open(MANIFEST_RAW, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f, fieldnames=["filepath", "class", "hash"], delimiter="\t"
        )
        writer.writeheader()
        writer.writerows(rows)

    print(f"âœ… Step1: manifest_raw.tsv ìƒì„± ì™„ë£Œ ({len(rows)}ê°œ)")


# ============================================================
# 4) Step 2 â€” ì´ë¯¸ì§€ ì •ê·œí™” & manifest_normalized.tsv
# ============================================================
def normalize_images(min_size=256, target_max=768):

    df = pd.read_csv(MANIFEST_RAW, sep="\t")
    out_rows = []

    for _, row in df.iterrows():
        src = Path(row.filepath)
        if not src.exists():
            continue

        try:
            im = Image.open(src).convert("RGB")
            w, h = im.size

            # ì‘ì€ ì´ë¯¸ì§€ ì œê±°
            if min(w, h) < min_size:
                continue

            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆ
            scale = target_max / max(w, h)
            if scale < 1.0:
                im = im.resize((int(w * scale), int(h * scale)))

            # ì €ì¥ ê²½ë¡œ
            cls_dir = NORMALIZED_DIR / row["class"]
            cls_dir.mkdir(parents=True, exist_ok=True)
            out_path = cls_dir / f"{row['hash']}.jpg"

            im.save(out_path, "JPEG", quality=92, optimize=True)

            out_rows.append({
                "filepath": str(out_path),
                "class": row["class"],
                "hash": row["hash"]
            })

        except Exception:
            continue

    pd.DataFrame(out_rows).to_csv(MANIFEST_NORMALIZED, sep="\t", index=False)

    print(f"âœ… Step2: normalized ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ ({len(out_rows)}ê°œ)")
    print(f"ğŸ“„ íŒŒì¼: {MANIFEST_NORMALIZED}")


# ============================================================
# 5) Step 3 â€” íë¦¿í•œ ì´ë¯¸ì§€ ì œê±° + manifest_clean.tsv
# ============================================================
def variance_of_laplacian(image):
    """íë¦¿í•œ ì´ë¯¸ì§€ íŒë³„ìš© ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚° ê³„ì‚°"""
    return cv2.Laplacian(image, cv2.CV_64F).var()


def clean_blurry_images(blur_threshold=20.0):

    df = pd.read_csv(MANIFEST_NORMALIZED, sep="\t")
    print(f"ì •ê·œí™”ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(df)}")

    # í•´ì‹œ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    df = df.drop_duplicates(subset="hash", keep="first")
    print(f"ì¤‘ë³µ ì œê±° í›„: {len(df)}")

    keep_rows = []

    for _, row in df.iterrows():
        p = row.filepath
        img = cv2.imdecode(np.fromfile(p, dtype=np.uint8), cv2.IMREAD_GRAYSCALE)
        if img is None:
            continue

        # íë¦¿í•œ ì´ë¯¸ì§€ ì œê±°
        if variance_of_laplacian(img) < blur_threshold:
            continue

        keep_rows.append(row)

    pd.DataFrame(keep_rows).to_csv(MANIFEST_CLEAN, sep="\t", index=False)

    print(f"âœ… Step3: blur ì œê±° í›„ ë‚¨ì€ ì´ë¯¸ì§€: {len(keep_rows)}")
    print(f"ğŸ“„ íŒŒì¼: {MANIFEST_CLEAN}")


# ============================================================
# 6) Main â€” ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    print("ğŸ”§ SkinAI ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    build_manifest_raw()
    normalize_images()
    clean_blurry_images()
    print("ğŸ‰ ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ!")